{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Diagnostics in Statsmodels\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far, you have looked mainly at R-Squared values along with some visualization techniques to confirm if the data and residuals fit the regression assumptions. Now, you'll look at some statistical procedures to further understand your model and results. You'll be looking at the results obtained in the regression analysis outcomes for the advertising dataset in the previous lab. \n",
    "\n",
    "*Note: Some of the terms in this lesson highlighting underlying statistical testing concepts will be new to you. These terms will be covered in detail in later sections. Here, the focus will be on running and interpreting the results of these tests in a regression context.*\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Use Q-Q plots for check for the normality in residual errors\n",
    "* Use the Jarque-Bera test for normal distribution of residuals\n",
    "* Check for heteroscedasticity using the Goldfeld-Quandt test to check whether variance is the same in 2 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "Regression diagnostics is a set of procedures available for regression analysis that assess the validity of a model in a number of different ways. \n",
    "\n",
    "This could be:\n",
    "- An exploration of the model's underlying statistical assumptions\n",
    "- An examination of the structure of the model by considering formulations that have less, more or different explanatory variables\n",
    "- A study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model's predictions\n",
    "\n",
    "For a thorough overview, you can go and have a look at the [wikipedia page on regression diagnostics.](https://en.wikipedia.org/wiki/Regression_diagnostic)\n",
    "\n",
    "Here we'll revisit some of the methods you've already seen, along with some new tests and how to interpret them. \n",
    "\n",
    "## Normality Check (Q-Q plots) \n",
    "You've already seen Q-Q Plots as a measure to check for normality (or, by extension, any other distribution). \n",
    "\n",
    "Q-Q plots are also referred to as normal density plots when used with a standard normal quantiles. These plots are a good way to inspect the distribution of model errors. You saw this earlier with the small height-weight data set. Now, let's plot a Q-Q for the residuals in the `sales ~ TV` and the `sales ~ radio` models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "data = pd.read_csv('advertising.csv', index_col=0)\n",
    "f = 'sales~TV'\n",
    "f2 = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "model2 = smf.ols(formula=f2, data=data).fit()\n",
    "\n",
    "resid1 = model.resid\n",
    "resid2 = model2.resid\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True)\n",
    "fig = sm.graphics.qqplot(resid2, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Q-Q Plots are a direct visual assessment of how well our residuals match what we would expect from a normal distribution. \n",
    "\n",
    "In the Q-Q plots above, you can see that residuals are better normally distributed for TV than for radio. \n",
    "\n",
    "You can also spot an outlier in the left tail of radio residuals. Dealing with this might help improve the fit of the model. Outliers, skew, heavy and light-tailed aspects of distributions (all violations of normality) can be assessed from Q-Q plots. It might require a bit of practice before you can truly start to interpret them. \n",
    "\n",
    "The images below show you how to relate a histogram to their respective Q-Q Plots.\n",
    "\n",
    "![](images/qq1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality Check (Jarque-Bera Test)\n",
    "\n",
    "The Jarque-Bera (JB) test is a test for normality. This test is usually used for large data sets, because other tests like Q-Q Plots can become unreliable when your sample size is large.\n",
    "\n",
    ">The Jarque-Bera test inspects the skewness and kurtosis of data to see if it matches a normal distribution. It is a common method for inspecting errors distribution in regression as shown below. \n",
    "\n",
    "$$JB = n *\\Bigl(\\dfrac{S^2}{6} + \\dfrac{(K â€“ 3)^2}{24}\\Bigr)$$\n",
    "\n",
    "\n",
    "Here, $n$ is the sample size, $S$ is the sample skewness coefficient and $K$ is the sample kurtosis.\n",
    "\n",
    "Here is how you use JB in statsmodels. A JB value of roughly 6 or higher indicates that errors are not normally distributed. In other words, this means that the normality null hypothesis has been rejected at the $5\\%$ significance level. A value close to 0 on the contrary, indicates the data $is$ normally distributed. We have already seen the JB test using `model.summary()`. The code below shows you how to run this test on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JB test for TV\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(model.resid)\n",
    "list(zip(name, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a JB value = 0.67, which is pretty low (and in favor of normality), and the p-value of 0.71 is quite high to reject the null hypothesis for normality. Additionally, the kurtosis is below 3, where a kurtosis higher than 3 indicates heavier tails than a normal distribution. The skewness values however show that underlying data is moderately skewed. Let's see what happens if we look at the `radio` residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JB test for radio\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test2 = sms.jarque_bera(model2.resid)\n",
    "list(zip(name, test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where The TV residuals showed to be close to normality, the JB results for radio are considerably worse. More-over, a JB p-value much smaller than 0.05 indicates that the normality assumption should definitely be rejected.\n",
    "\n",
    "These results show that even when in the Q-Q plots the results seemed moderately different, the JB test could shed new light on the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Hetereoscadasticity (Goldfeld-Quandt test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. The GQ test checks if you can define a point that can be used to **differentiate** the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\n",
    "\n",
    "Here is an in-depth visual explanation on how this test is performed.\n",
    "\n",
    "\n",
    "In the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroskedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\n",
    "\n",
    "<img src=\"images/gq1.png\" width=500>\n",
    "\n",
    "Here is a brief description of involved steps:\n",
    "\n",
    "* Order the data in ascending order \n",
    "* Split your data into _three_ parts and drop values in the middle part.\n",
    "* Run separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\n",
    "* Calculate the ratio of the Residual sum of squares of two parts.\n",
    "* Apply the F-test. \n",
    "\n",
    "(F-test will be covered later in the syllabus. [Here](https://en.wikipedia.org/wiki/F-test) is a quick introduction)\n",
    "\n",
    "For now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small.\n",
    "However, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\n",
    "\n",
    "Here is how you can run this test in statsmodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Goldfeld Quandt test\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid, model.model.exog)\n",
    "list(zip(name, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Goldfeld Quandt test\n",
    "import statsmodels.stats.api as sms\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model2.resid, model2.model.exog)\n",
    "list(zip(name, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis for the GQ test is homoskedasticity. The larger the F-statistic, the more evidence we will have against the homoskedasticity assumption and the more likely we have heteroskedasticity (different variance for the two groups). \n",
    "\n",
    "The p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null-hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "In this lesson, you learned a few methods to check for regression assumptions in addition to the visual methods learned earlier. An understanding and hands-on experience with visual as well as statistical techniques to check your regression analysis will provide you with a good set of tools to run more detailed regression experiments later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
